\docappendix{Авторская справка}\label{ax:authornote}
\input{pages/authornote.example.tex}


\docappendix{Листинг кода}\label{ax:code}

\begin{lstlisting}
    # Component for searching car models using the autoteka.ru website 
    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait  # Импортируем WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import time

    # Path to driver
    driver_path = 'drv/chromedriver-win64/chromedriver.exe'

    # URL link to the page
    website_url = 'https://autoteka.ru/'

    # Initializing the driver
    chrome_service = webdriver.chrome.service.Service(driver_path)
    driver = webdriver.Chrome(service=chrome_service)

    # Function for parsing
    def site(number):
        input = number
        driver.get(website_url)

        input_field = driver.find_element(By.NAME, 'identifier')
        input_field.send_keys(input)
        input_field.send_keys(Keys.ENTER)

        WebDriverWait(driver, 15).until(EC.url_changes(website_url))

        time.sleep(5)

        text_element = driver.find_element(By.CLASS_NAME, 'pit4K')
        text = text_element.text

        time.sleep(3)

        lines = text.split('\n')
        result = ' '.join(lines[:2])

        driver.quit()

        return result.split(',')[0].strip()
\end{lstlisting}

\begin{lstlisting}
    # Component for analyzing forms
    from imutils.perspective import four_point_transform
    from imutils import contours
    import numpy as np
    import argparse
    import imutils
    import cv2
    import csv
    import re
    import pytesseract
    
    # Define the correct answers for the exam (placeholder)
    ANSWER_KEY = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
    
    # Path to Tesseract executable
    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'
    
    # Tesseract OCR configuration
    custom_config = r'--oem 3 --psm 6'
    
    def process_exam(image_path):
        # Read the input image
        image = cv2.imread(image_path)
        
        # Convert the image to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Apply GaussianBlur to reduce noise and detail in the image
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Use Canny edge detector to find edges in the image
        edged = cv2.Canny(blurred, 75, 200)
    
        # Find contours in the edged image
        cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        docCnt = None
    
        # Ensure at least one contour was found
        if len(cnts) > 0:
            # Sort the contours by area, keeping only the largest one
            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
            for c in cnts:
                # Approximate the contour
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.02 * peri, True)
                
                # If the approximated contour has four points, assume it's the paper
                if len(approx) == 4:
                    docCnt = approx
                    break
    
        # Apply a perspective transform to obtain a top-down view of the paper
        paper = four_point_transform(image, docCnt.reshape(4, 2))
        warped = four_point_transform(gray, docCnt.reshape(4, 2))
    
        # Apply a binary threshold to the warped image
        thresh = cv2.threshold(warped, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
    
        # Find contours in the thresholded image
        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        questionCnts = []
    
        # Loop over the contours
        for c in cnts:
            (x, y, w, h) = cv2.boundingRect(c)
            ar = w / float(h)
            
            # Filter the contours to find those that correspond to question bubbles
            if (w >= 50) and (h >= 50) and (ar >= 0.9) and (ar <= 1.1):
                questionCnts.append(c)
    
        # Sort the question contours top-to-bottom
        questionCnts = contours.sort_contours(questionCnts, method="top-to-bottom")[0]
        results = []
    
        # Loop over the question groups
        for (q, i) in enumerate(np.arange(0, len(questionCnts), 2)):
            cnts = contours.sort_contours(questionCnts[i:i + 2])[0]
            bubbled = None
            question_result = 0
    
            # Loop over the sorted contours
            for (j, c) in enumerate(cnts):
                mask = np.zeros(thresh.shape, dtype="uint8")
                cv2.drawContours(mask, [c], -1, 255, -1)
    
                mask = cv2.bitwise_and(thresh, thresh, mask=mask)
                total = cv2.countNonZero(mask)
                if bubbled is None or total > bubbled[0]:
                    bubbled = (total, j)
    
            color = (0, 0, 255)
            k = ANSWER_KEY[q]
    
            if k == bubbled[1]:
                question_result = 1
                color = (0, 255, 0)
    
            cv2.drawContours(paper, [cnts[k]], -1, color, 3)
            results.append(question_result)
    
        print(results)
    
        cv2.imshow("Proc", paper)
        cv2.waitKey(0)
    
        return results
    
    def get_age(image_path):
        # Read the input image
        image = cv2.imread(image_path)
        
        # Convert the image to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Apply GaussianBlur to reduce noise and detail in the image
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Use Canny edge detector to find edges in the image
        edged = cv2.Canny(blurred, 75, 200)
    
        # Find contours in the edged image
        cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        docCnt = None
    
        # Ensure at least one contour was found
        if len(cnts) > 0:
            # Sort the contours by area, keeping only the largest one
            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
            for c in cnts:
                # Approximate the contour
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.02 * peri, True)
                
                # If the approximated contour has four points, assume it's the paper
                if len(approx) == 4:
                    docCnt = approx
                    break
    
        # Apply a perspective transform to obtain a top-down view of the paper
        paper = four_point_transform(image, docCnt.reshape(4, 2))
    
        # Further process the transformed image to find the age
        gray = cv2.cvtColor(paper, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        edged = cv2.Canny(blurred, 75, 200)
    
        cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        docCnt = None
    
        if len(cnts) > 0:
            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
            for c in cnts:
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.02 * peri, True)
                if len(approx) == 4:
                    docCnt = approx
                    break
    
        paper = four_point_transform(paper, docCnt.reshape(4, 2))
    
        gray = cv2.cvtColor(paper, cv2.COLOR_BGR2GRAY)
        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
        
        # Use OCR to extract text from the thresholded image
        text = pytesseract.image_to_string(thresh, config=custom_config)
        
        # Find all two-digit numbers in the extracted text
        number = re.findall(r'\b\d+\b', text)
        two_digit_numbers = ' '.join(num for num in number if len(num) == 2)
        print(two_digit_numbers)
        return two_digit_numbers
    
    def write_to_csv(filename, results):
        # Append the results to a CSV file
        with open(filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(results)
    
    def blank(path_in, path_out, model):
        print(path_in)
        print(path_out)
        
        # Process the exam and get the age
        results = process_exam(path_in)
        age = get_age(path_in)
        
        # Insert model and age to the results
        results.insert(0, int(age))
        results.insert(0, model)
        
        print(results)
        write_to_csv(path_out, results)
        print("done")
    
    def main():
        # Set up argument parser
        ap = argparse.ArgumentParser()
        ap.add_argument("-i", "--image", required=True, help="path to the input image")
        ap.add_argument("-o", "--output", required=True, help="path to save the output CSV file")
        args = vars(ap.parse_args())
        
        # Process the exam and get the age
        results = process_exam(args["image"])
        age = get_age(args["image"])
        
        # Insert age to the results
        results.insert(0, int(age))
        print(results)
        
        # Write results to the CSV file
        write_to_csv(args["output"], results)
    
    if __name__ == "__main__":
        main()    
\end{lstlisting}

\begin{lstlisting}
    # Component for working with a trained model
    import numpy as np
    import pandas as pd
    import tensorflow as tf
    from sklearn.preprocessing import OneHotEncoder
    import os

    # Transforming new data
    def preprocess_new_data(new_data, full_data):
        # Convert categorical column "Model" to numeric values ​​using OneHotEncoding
        encoder = OneHotEncoder()
        encoder.fit(full_data[['Model']])
        model_encoded = encoder.transform(new_data[['Model']]).toarray()
        
        # Normalization of numerical data (age)
        age = new_data[['Age']].values
        age_normalized = (age - np.mean(full_data['Age'])) / np.std(full_data['Age'])

        # Merging the transformed data
        X_new = np.concatenate([age_normalized, model_encoded], axis=1)
        
        return X_new

    # Receiving Predictions
    def get_predictions(X_new, model_p):
        predictions = model_p.predict(X_new)
        # binary_predictions = np.round(predictions)
        return predictions

    # Output results
    def print_results(binary_predictions):
        # print("New data:")
        # print(new_data)
        print("\nPredictions:")
        np.set_printoptions(threshold=np.inf)
        print(binary_predictions)

    def main(model, age, model_pred):
        
        # Example of new data for testing
        new_data = pd.DataFrame({'Model': [model], 'Age': [int(age)]})

        # Transforming new data
        X_new = preprocess_new_data(new_data, full_data)

        if not os.path.isfile(model_pred):
            raise FileNotFoundError(f"Файл {model_pred} не найден")

        # Check file extension
        if not model_pred.endswith('.keras'):
            raise ValueError("Неправильный формат файла. Ожидался файл с расширением .keras")

        model_p = tf.keras.models.load_model(model_pred)

        # Getting predictions for just one example
        binary_predictions = get_predictions(X_new, model_p)

        art_prediction = binary_predictions[0][0]
        sport_prediction = binary_predictions[0][1]
        book_films_prediction = binary_predictions[0][2]
        science_prediction = binary_predictions[0][3]
        travel_prediction = binary_predictions[0][4]
        cooking_prediction = binary_predictions[0][5]
        politics_prediction = binary_predictions[0][6]

        # Writing to an array
        predictions_array = [art_prediction, sport_prediction, book_films_prediction, science_prediction, travel_prediction, cooking_prediction, politics_prediction]

        return predictions_array
\end{lstlisting}

\begin{lstlisting}
    # Component for training neural networks
    import pandas as pd
    import numpy as np
    import tensorflow as tf
    from tensorflow import keras
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import OneHotEncoder
    from scikeras.wrappers import KerasClassifier
    from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
    from tqdm.keras import TqdmCallback

    # Load the dataset
    data = pd.read_csv('datanew2.csv')

    # Convert the categorical "Model" column to numerical values using OneHotEncoder
    encoder = OneHotEncoder()
    model_encoded = encoder.fit_transform(data[['Model']]).toarray()

    # Normalize the numerical "Age" column
    age = data[['Age']].values
    age_normalized = (age - np.mean(age)) / np.std(age)

    # Combine the normalized age data and the encoded model data
    X = np.concatenate([age_normalized, model_encoded], axis=1)

    # Extract the target variables (interests)
    y = data[['Art', 'Sport', 'Book/Films', 'Science', 'Travel', 'Cooking', 'Politics']].values

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Function to create the model with specified parameters
    def create_model(optimizer='adam', num_hidden_layers=2, num_neurons=20, activation='relu', dropout_rate=0.2, learning_rate=0.001):
        # Input layer
        inputs = tf.keras.Input(shape=(X.shape[1],))
        
        # First hidden layer
        x = tf.keras.layers.Dense(num_neurons, activation=activation)(inputs)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        
        # Additional hidden layers based on the parameter num_hidden_layers
        for _ in range(num_hidden_layers - 1):
            x = tf.keras.layers.Dense(num_neurons, activation=activation)(x)
            x = tf.keras.layers.Dropout(dropout_rate)(x)
        
        # Output layer with sigmoid activation for multi-label classification
        outputs = tf.keras.layers.Dense(7, activation='sigmoid')(x)
        
        # Create the model
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        
        # Set the optimizer based on the specified parameter
        if optimizer == 'adam':
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        elif optimizer == 'rmsprop':
            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
        elif optimizer == 'sgd':
            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
        
        # Compile the model
        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
        return model

    # Create a KerasClassifier with the create_model function
    model = KerasClassifier(model=create_model, verbose=0, epochs=100)

    # Define the grid of parameters for GridSearchCV
    param_grid = {
        'model__num_hidden_layers': [2, 3, 4, 5],
        'model__num_neurons': [20, 30, 40],
        'model__activation': ['relu', 'tanh'],
        'model__dropout_rate': [0.1, 0.2],
        'model__optimizer': ['adam', 'rmsprop', 'sgd'],
        'model__learning_rate': [0.001, 0.01, 0.1]
    }

    # Create a GridSearchCV object
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)

    # Fit the GridSearchCV object to the training data
    grid_search.fit(X_train, y_train)

    # Print the best parameters found by GridSearchCV
    print("Best: %f using %s" % (grid_search.best_score_, grid_search.best_params_))

    # Get the best parameters and create a model with them
    best_params = grid_search.best_params_
    best_model = create_model(
        optimizer=best_params['model__optimizer'],
        num_hidden_layers=best_params['model__num_hidden_layers'],
        num_neurons=best_params['model__num_neurons'],
        activation=best_params['model__activation'],
        dropout_rate=best_params['model__dropout_rate'],
        learning_rate=best_params['model__learning_rate']
    )

    # Define callbacks for early stopping, model checkpointing, and TQDM progress bar
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint('model.keras', save_best_only=True, monitor='val_loss', mode='min')
    tqdm_callback = TqdmCallback(verbose=1)

    # Train the best model with the callbacks
    history = best_model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=100,
        callbacks=[early_stopping, model_checkpoint, tqdm_callback],
        verbose=0
    )

    # Evaluate the model on the test set
    loss, accuracy = best_model.evaluate(X_test, y_test)
    print(f'Test Accuracy: {accuracy}')
\end{lstlisting}

\begin{lstlisting}
    # Vehicle Number Plate Recognition Component
    import os
    from datetime import timedelta
    from pathlib import Path
    import matplotlib.pyplot as plt
    import cv2
    import numpy as np
    import re
    import imutils
    import pytesseract
    import pytesseract as tess
    tess.pytesseract.tesseract_cmd = r'Tesseract-OCR\tesseract.exe'
    from PyQt6 import uic
    from PyQt6.QtGui import QPixmap
    from PyQt6.QtWidgets import QApplication, QFileDialog, QMessageBox
    from imutils import contours
    from moviepy.editor import VideoFileClip
    import json
    import sqlite3
    import itertools
    import tensorflow as tf
    from skimage.feature import canny
    from skimage.transform import hough_line, hough_line_peaks, rotate
    from skimage.color import rgb2gray

    import matplotlib.gridspec as gridspec
    import cv2

    import autoteka
    import test_blank

    # Constants for saving frames per second from video
    SAVING_FRAMES_PER_SECOND = 1

    # Load the UI design file
    Form, Window = uic.loadUiType("Parking.ui")

    # Initialize the application and load the UI window
    app = QApplication([])
    window = Window()
    form = Form()
    form.setupUi(window)
    window.show()

    # Global variables to store the paths of video, image, and the result
    video_file = ''
    image_file = ''
    result = ''
    arr = []

    # Function to format the time delta for naming frames
    def format_timedelta(td):
        result = str(td)
        try:
            result, ms = result.split(".")
        except ValueError:
            return result + ".00".replace(":", "-")

        ms = round(int(ms) / 10000)
        return f"{result}.{ms:02}".replace(":", "-")

    # Function to load the video file
    def load():
        global video_file
        video_file = QFileDialog.getOpenFileName()
        path = Path(video_file[0])
        video_file = path.name
        print("load")

    # Function to split the video into frames
    def split():
        video_clip = VideoFileClip(video_file)
        filename, _ = os.path.splitext(video_file)

        if not os.path.isdir(filename):
            os.mkdir(filename)

        saving_frames_per_second = min(video_clip.fps, SAVING_FRAMES_PER_SECOND)
        step = 1 / video_clip.fps if saving_frames_per_second == 0 else 1 / saving_frames_per_second

        for current_duration in np.arange(0, video_clip.duration, step):
            frame_duration_formatted = format_timedelta(timedelta(seconds=current_duration)).replace(":", "-")
            frame_filename = os.path.join(filename, f"frame{frame_duration_formatted}.jpg")

            video_clip.save_frame(frame_filename, current_duration)
        print("split")

    # Function to check the format of the recognized license plate
    def check_format(variable):
        pattern = r'^[A-Z]\d{3}[A-Z]{2}\d{3}$'
        pattern2 = r'^[A-Z]\d{3}[A-Z]{2}\d{2}$'
        if re.match(pattern, variable) or re.match(pattern2, variable):
            return True
        else:
            return False

    # Function to choose an image file for recognition
    def choose():
        global image_file
        image_file = QFileDialog.getOpenFileName()
        image_file = image_file[0]
        form.label_6.setPixmap(QPixmap(image_file))
        form.label_6.setScaledContents(True)
        print("choose")

    # Function to recognize text from the car plate
    def carplate_text():
        global image_file

        image0 = cv2.imread(image_file)
        image_height, image_width, _ = image0.shape
        image = cv2.resize(image0, (1024, 1024))
        image = image.astype(np.float32)
        paths = 'model/recognize/model_resnet.tflite'
        interpreter = tf.lite.Interpreter(model_path=paths)
        interpreter.allocate_tensors()
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        X_data1 = np.float32(image.reshape(1, 1024, 1024, 3))

        interpreter.set_tensor(input_details[0]['index'], X_data1)
        interpreter.invoke()
        detection = interpreter.get_tensor(output_details[0]['index'])
        net_out_value2 = interpreter.get_tensor(output_details[1]['index'])
        net_out_value3 = interpreter.get_tensor(output_details[2]['index'])
        net_out_value4 = interpreter.get_tensor(output_details[3]['index'])

        img = image0
        razmer = img.shape

        img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img3 = img[:, :, :]

        number = 0
        while number < len(detection[0][number]) and detection[0, number, 0] > 0.9:
            number = number + 1

        box_x = int(detection[0, number, 0] * image_height)
        box_y = int(detection[0, number, 1] * image_width)
        box_width = int(detection[0, number, 2] * image_height)
        box_height = int(detection[0, number, 3] * image_width)

        cv2.rectangle(img2, (box_y, box_x), (box_height, box_width), (230, 230, 21), thickness=5)

        net_out_value3

        image = image0[box_x:box_width, box_y:box_height, :]
        img2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        grayscale = rgb2gray(image)
        edges = canny(grayscale, sigma=3.0)
        out, angles, distances = hough_line(edges)
        h, theta, d = out, angles, distances
        angle_step = 0.5 * np.diff(theta).mean()
        d_step = 0.5 * np.diff(d).mean()
        bounds = [np.rad2deg(theta[0] - angle_step),
                np.rad2deg(theta[-1] + angle_step),
                d[-1] + d_step, d[0] - d_step]

        _, angles_peaks, _ = hough_line_peaks(out, angles, distances, num_peaks=20)
        angle = np.mean(np.rad2deg(angles_peaks))

        if 0 <= angle <= 90:
            rot_angle = angle - 90
        elif -45 <= angle < 0:
            rot_angle = angle - 90
        elif -90 <= angle < -45:
            rot_angle = 90 + angle
        if abs(rot_angle) > 20:
            rot_angle = 0

        rotated = rotate(image, rot_angle, resize=True) * 255
        rotated = rotated

        rotated1 = rotated[:, :, :]
        if rotated.shape[1] / rotated.shape[0] < 2:
            minus = np.abs(int(np.sin(np.radians(rot_angle)) * rotated.shape[0]))
            rotated1 = rotated[minus:-minus, :, :]
            print(minus)

        lab = cv2.cvtColor(rotated1, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)

        letters = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'E', 'H', 'K', 'M', 'O', 'P', 'T', 'X',
                'Y']

        def decode_batch(out):
            ret = []
            for j in range(out.shape[0]):
                out_best = list(np.argmax(out[j, 2:], 1))
                out_best = [k for k, g in itertools.groupby(out_best)]
                outstr = ''
                for c in out_best:
                    if c < len(letters):
                        outstr += letters[c]
                ret.append(outstr)
            return ret

        paths = 'model/recognize/model1_nomer.tflite'
        interpreter = tf.lite.Interpreter(model_path=paths)
        interpreter.allocate_tensors()

        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        img = final
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (128, 64))
        img = img.astype(np.float32)
        img /= 255

        img1 = img.T
        img1.shape
        X_data1 = np.float32(img1.reshape(1, 128, 64, 1))
        input_index = (interpreter.get_input_details()[0]['index'])
        interpreter.set_tensor(input_details[0]['index'], X_data1)

        interpreter.invoke()

        net_out_value = interpreter.get_tensor(output_details[0]['index'])
        pred_texts = decode_batch(net_out_value)
        pred_texts

        fig = plt.figure(figsize=(10, 10))
        outer = gridspec.GridSpec(2, 1, wspace=10, hspace=0.1)
        ax1 = plt.Subplot(fig, outer[0])
        fig.add_subplot(ax1)
        ax2 = plt.Subplot(fig, outer[1])
        fig.add_subplot(ax2)
        return pred_texts[0]

    # Function to recognize the license plate and display information
    def recognize():
        global result
        result = carplate_text().upper()
        print(result)
        if result == '':
            QMessageBox.information(None, "Ошибка распознания", "НЕ РАСПОЗНАНО")
        elif check_format(result) != True:
            QMessageBox.information(None, "Ошибка формата", "Результат:" + result)
        else:
            form.label_7.setText(result)

            count = 0
            for i in range(len(arr)):
                if arr[i][0] == result:
                    count += 1

            if count == 0:
                arr.append([result, 1])
            else:
                for i in range(len(arr)):
                    if arr[i][0] == result:
                        arr[i][1] += 1

            for i in range(len(arr)):
                if arr[i][0] == result:
                    form.label_2.setText(str(arr[i][1]))
                    if arr[i][1] < 5:
                        form.label_5.setText('0%')
                    elif 5 <= arr[i][1] < 10:
                        form.label_5.setText('5%')
                    elif 10 <= arr[i][1] < 20:
                        form.label_5.setText('10%')
                    else:
                        form.label_5.setText('15%')
        
        model = autoteka.site(result)
        print(model)
        print("recognize")

        file_path, _ = QFileDialog.getOpenFileName()
        test_blank.blank(file_path, 'data/data.csv', model)

    # Connect UI buttons to their respective functions
    form.pushButton.clicked.connect(load)
    form.pushButton_2.clicked.connect(split)
    form.pushButton_3.clicked.connect(choose)
    form.pushButton_4.clicked.connect(recognize)

    # Start the application event loop
    app.exec()
\end{lstlisting}

\begin{lstlisting}
    #Interface
    # Importing necessary modules and libraries
    from PyQt6.QtWidgets import (
        QApplication, 
        QWidget, 
        QPushButton, 
        QLabel, 
        QVBoxLayout, 
        QTextEdit, 
        QTableWidget, 
        QTableWidgetItem,
        QTabWidget,
        QMainWindow,
        QFileDialog,
        QMessageBox
    )
    from PyQt6 import QtCore

    # Constant for saving frames per second from a video
    SAVING_FRAMES_PER_SECOND = 1

    # List of interest categories
    list_int = ["Искусство", "Спорт", "Книги/Фильмы", "Наука", "Путешествия", "Кулинария", "Политика"]

    # Global variables for storing file paths and results
    video_file = ''
    image_file = ''
    result = ''
    arr = []

    # Main window class inheriting from QMainWindow
    class MainWindow(QMainWindow):
        def __init__(self):
            super().__init__()

            self.setWindowTitle("Предсказание интересов")
            self.setGeometry(400, 250, 750, 500)

            # Creating a tab widget and setting it as the central widget
            self.tabs = QTabWidget()
            self.setCentralWidget(self.tabs)

            # Creating tabs
            self.create_tabs()

            self.model_pred = ""

        # Method to create tabs
        def create_tabs(self):
            # Creating the first tab for data collection
            tab1 = QWidget()
            
            self.pushButton = QPushButton(tab1)
            self.pushButton.setGeometry(QtCore.QRect(10, 10, 141, 41))
            self.pushButton.setObjectName("pushButton")
            self.pushButton.clicked.connect(self.load)
            self.pushButton.setText("Загрузить видео")

            self.pushButton_2 = QPushButton(tab1)
            self.pushButton_2.setGeometry(QtCore.QRect(10, 60, 141, 41))
            self.pushButton_2.setObjectName("pushButton_2")
            self.pushButton_2.clicked.connect(self.split)
            self.pushButton_2.setText("Разбить на кадры")

            self.pushButton_3 = QPushButton(tab1)
            self.pushButton_3.setGeometry(QtCore.QRect(10, 110, 141, 41))
            self.pushButton_3.setObjectName("pushButton_3")
            self.pushButton_3.clicked.connect(self.choose)
            self.pushButton_3.setText("Выбрать номер")

            self.pushButton_4 = QPushButton(tab1)
            self.pushButton_4.setGeometry(QtCore.QRect(10, 160, 141, 41))
            self.pushButton_4.setObjectName("pushButton_4")
            self.pushButton_4.clicked.connect(self.recognize)
            self.pushButton_4.setText("Распознать номер")

            self.label_6 = QLabel(tab1)
            self.label_6.setGeometry(QtCore.QRect(170, 10, 441, 241))
            self.label_6.setText("")
            self.label_6.setAlignment(QtCore.Qt.AlignmentFlag.AlignCenter)
            self.label_6.setObjectName("label_6")

            self.label_3 = QLabel(tab1)
            self.label_3.setGeometry(QtCore.QRect(200, 340, 71, 21))
            self.label_3.setText("Номер:")
            self.label_3.setObjectName("label_3")

            self.label_7 = QLabel(tab1)
            self.label_7.setGeometry(QtCore.QRect(290, 340, 161, 31))
            self.label_7.setText("")
            self.label_7.setObjectName("label_7")

            # Creating the second tab for neural network training
            tab2 = QWidget()
            tab2_layout = QVBoxLayout()

            self.pushButton_csv = QPushButton(tab2)
            self.pushButton_csv.setText("Выбрать CSV файл")
            self.pushButton_csv.setFixedSize(750, 40)
            self.pushButton_csv.clicked.connect(self.choose_csv_file)
            tab2_layout.addWidget(self.pushButton_csv)

            self.pushButton_learn = QPushButton(tab2)
            self.pushButton_learn.setText("Обучить")
            self.pushButton_learn.setFixedSize(750, 40)
            self.pushButton_learn.clicked.connect(self.learn)
            tab2_layout.addWidget(self.pushButton_learn)

            self.label_csv = QLabel(tab2)
            self.label_csv.setText("CSV файл не выбран")
            tab2_layout.addWidget(self.label_csv)

            tab2.setLayout(tab2_layout)

            # Creating the third tab for prediction
            tab3 = QWidget()
            tab3_layout = QVBoxLayout()

            self.label_model = QLabel('Введите модель', tab3)
            self.label_age = QLabel('Введите возраст', tab3)

            self.text_edit_model = QTextEdit(tab3)
            self.text_edit_model.setFixedSize(750, 30)
            
            self.text_edit_age = QTextEdit(tab3)
            self.text_edit_age.setFixedSize(750, 30)

            self.table = QTableWidget(tab3)
            self.table.setColumnCount(7)
            self.table.setRowCount(2)

            for col in range(7):
                item = QTableWidgetItem(list_int[col])
                self.table.setItem(0, col, item)

            self.button_predict = QPushButton('Получить', tab3)
            self.button_predict.setFixedSize(750, 40)
            self.button_predict.clicked.connect(self.proc)

            self.button_choose_file = QPushButton('Выбрать файл модели', tab3)
            self.button_choose_file.setFixedSize(750, 40)
            self.button_choose_file.clicked.connect(self.choose_model_file)

            # Adding widgets to the third tab
            tab3_layout.addWidget(self.label_model)
            tab3_layout.addWidget(self.text_edit_model)
            tab3_layout.addWidget(self.label_age)
            tab3_layout.addWidget(self.text_edit_age)
            tab3_layout.addWidget(self.button_choose_file)
            tab3_layout.addWidget(self.table)
            tab3_layout.addWidget(self.button_predict)

            tab3.setLayout(tab3_layout)

            # Adding tabs to the QTabWidget
            self.tabs.addTab(tab1, "Сбор данных")
            self.tabs.addTab(tab2, "Обучение нейросети")
            self.tabs.addTab(tab3, "Предсказание")

        # Placeholder method for learning
        def learn(path):
            pass

        # Method to choose a CSV file
        def choose_csv_file(self):
            file_dialog = QFileDialog()
            options = file_dialog.options()
            file_name, _ = QFileDialog.getOpenFileName(self, "Выберите CSV файл", "", "CSV Files (*.csv);;All Files (*)", options=options)
            if file_name:
                self.csv_file = file_name
                self.label_csv.setText(f"Выбранный CSV файл: {file_name}")
            print(self.csv_file)

        # Method to choose a model file
        def choose_model_file(self):
            file_dialog = QFileDialog()
            options = file_dialog.options()
            file_name, _ = QFileDialog.getOpenFileName(self, "Выберите файл модели", "", "Model Files (*.keras);;All Files (*)", options=options)
            if file_name:
                self.model_pred = file_name 
            print(self.model_pred)
            
        # Method to process the model and age input and get predictions
        def proc(self):
            model = self.text_edit_model.toPlainText()
            age = self.text_edit_age.toPlainText()
            if (model != "") & (age != "") & (self.model_pred != ""):
                predictions = test_single.main(model, age, self.model_pred)
                print(predictions)
                for col in range(7):
                    predictions[col] = float(predictions[col])
                    item = QTableWidgetItem(str("{:2.2f}".format(predictions[col] * 100)) + "%")
                    self.table.setItem(1, col, item)

    if __name__ == "__main__":
        app = QApplication(sys.argv)
                
        window = MainWindow()
        window.show()
                
        sys.exit(app.exec()) 
\end{lstlisting}


\docappendix[Справочное]{Библиографический список}

\begin{references}
	\item\label{ref:data-collection} Data Collection [Электронный ресурс] – Режим доступа: https://www.tutorialspoint.com/data-collection, свободный. – Загл. с экрана.

	\item\label{ref:data-analysis} Анализ данных [Электронный ресурс] – Режим доступа: https://en.wikipedia.org/wiki/Data\_analysis, свободный. – Загл. с экрана.

	\item\label{ref:data-visualization} Визуализация данных [Электронный ресурс] – Режим доступа: https://practicum.yandex.ru/blog/vizualizaciya-dannyh/, свободный. – Загл. с экрана.

	\item\label{ref:machine-learning} Машинное обучение: методы и способы [Электронный ресурс] – Режим доступа: https://www.osp.ru/cio/2018/05/13054535, свободный. – Загл. с экрана.

	\item\label{ref:marketing} Медведев П.М. Организация маркетинговой службы с нуля [Текст] ЗАО Издательский дом «Питер». 2005. – 224 с.: ил.

	\item\label{ref:res-net} Exploring ResNet50: An In-Depth Look at the Model Architecture and Code Implementation [Электронный ресурс] – Режим доступа: https://medium.com/@nitishkundu1993/exploring-resnet50-an-in-depth-look-at-the-model-architecture-and-code-implementation-d8d8fa67e46f, свободный. – Загл. с экрана.

	\item\label{ref:cnn-lstm} CNN-LSTM Architecture and Image Captioning [Электронный ресурс] – Режим доступа: https://medium.com/analytics-vidhya/cnn-lstm-architecture-and-image-captioning-2351fc18e8d7, свободный. – Загл. с экрана.
	
	\item\label{ref:neuron} Дюк В.А., Флегонтов А.В., Фомина И.К. Применение технологий интеллектуального анализа данных в естественнонаучных, технических и гуманитарных областях [Текст] Известия Российского государственного педагогического университета им. А.И. Герцена. 2011. No 138.

	\item\label{ref:back-error} Метод обратного распространения ошибки [Электронный ресурс] – Режим доступа: https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki, свободный. – Загл. с экрана.

	\item\label{ref:neuron-model} Neurons in Neural Networks [Электронный ресурс] – Режим доступа: https://www.baeldung.com/cs/neural-networks-neurons, свободный. – Загл. с экрана.

	\item\label{ref:nbc} Наивный байесовский классификатор [Электронный ресурс] – Режим доступа: https://ru.wikipedia.org/wiki/Наивный\_байесовский\_классификатор, свободный. – Загл. с экрана.

	\item\label{ref:log-reg} Пампел Фред, Цвиркун Дмитрий, Груздев Артем. Логистическая регрессия [Текст] ДМК-Пресс, 2023 г. – 218 с.: ил.

	
	\label{ref:total}
\end{references}
